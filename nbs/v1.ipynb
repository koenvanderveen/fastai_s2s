{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.text.all import *\n",
    "from fastai2.text.core import *\n",
    "from fastai2.text.core import _join_texts\n",
    "from fastai2.basics import *\n",
    "from fastai.text.models.qrnn import QRNN, QRNNLayer\n",
    "from util import *\n",
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Config().data_path/'giga-fren'\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(df, text_cols, n_workers=defaults.cpus, rules=None, mark_fields=None, out_col='text',\n",
    "                tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    \"Tokenize texts in `df[text_cols]` in parallel using `n_workers`\"\n",
    "    text_cols = L(text_cols)\n",
    "    #mark_fields defaults to False if there is one column of texts, True if there are multiple\n",
    "    if mark_fields is None: mark_fields = len(text_cols)>1\n",
    "    rules = L(ifnone(rules, defaults.text_proc_rules.copy()))\n",
    "    texts = _join_texts(df[text_cols], mark_fields=mark_fields)\n",
    "    outputs = L(parallel_tokenize(texts, tok_func, rules, n_workers=n_workers, **tok_kwargs)\n",
    "               ).sorted().itemgot(1)\n",
    "\n",
    "    other_cols = df.columns[~df.columns.isin(text_cols)]\n",
    "    res = df[other_cols].copy()\n",
    "    res[out_col] = outputs\n",
    "    return res,Counter(outputs.concat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'questions_easy.csv')\n",
    "df.head()\n",
    "df=df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok,count    = tokenize_df(df,     \"en\", out_col=\"en\")\n",
    "df_tok,count_fr = tokenize_df(df_tok, \"fr\", out_col=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(range_of(df_tok))\n",
    "dsrc   = DataSource(df_tok,\n",
    "                    splits=splits, tfms=[[attrgetter(\"en\"), Numericalize(make_vocab(count))],\n",
    "                                         [attrgetter(\"fr\"), Numericalize(make_vocab(count_fr))]],\n",
    "                    dl_type=SortedDL)\n",
    "\n",
    "# TODO: change sortedDL to sortishDL\n",
    "# TODO: create s2sdatabunch class\n",
    "dbch   = dsrc.databunch(before_batch=lambda items: pad_input(items, pad_fields=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbch.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqQRNN(nn.Module):\n",
    "    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, \n",
    "                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):\n",
    "        super().__init__()\n",
    "        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx\n",
    "        self.emb_enc  = emb_enc\n",
    "        self.emb_enc_drop = nn.Dropout(p_inp)\n",
    "        self.encoder  = QRNN(emb_enc.weight.size(1), n_hid, n_layers=n_layers, dropout=p_enc)\n",
    "        self.out_enc  = nn.Linear(n_hid, emb_enc.weight.size(1), bias=False)\n",
    "        self.hid_dp   = nn.Dropout(p_hid)\n",
    "        self.emb_dec  = emb_dec\n",
    "        self.decoder  = QRNN(emb_dec.weight.size(1), emb_dec.weight.size(1), n_layers=n_layers, dropout=p_dec)\n",
    "        self.out_drop = nn.Dropout(p_out)\n",
    "        self.out      = nn.Linear(emb_dec.weight.size(1), emb_dec.weight.size(0))\n",
    "        self.out.weight.data = self.emb_dec.weight.data\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self.encoder.reset()\n",
    "        self.decoder.reset()\n",
    "        bs,sl = inp.size()\n",
    "        hid   = self.initHidden(bs)\n",
    "        emb   = self.emb_enc_drop(self.emb_enc(inp))\n",
    "        enc_out, hid = self.encoder(emb, hid)\n",
    "        hid   = self.out_enc(self.hid_dp(hid))\n",
    "\n",
    "        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n",
    "        outs = []\n",
    "        for i in range(self.max_len):\n",
    "            emb      = self.emb_dec(dec_inp).unsqueeze(1)\n",
    "            out, hid = self.decoder(emb, hid)\n",
    "            out      = self.out(self.out_drop(out[:,0]))\n",
    "            dec_inp  = out.max(1)[1]\n",
    "            outs.append(out)\n",
    "            if (dec_inp==self.pad_idx).all(): break\n",
    "        return torch.stack(outs, dim=1)\n",
    "    \n",
    "    def initHidden(self, bs): return one_param(self).new_zeros(self.n_layers, bs, self.n_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once\n",
    "# en_vecs = ft.load_model(str((path/'cc.en.300.bin')))\n",
    "# emb_enc = create_emb(en_vecs, dsrc.vocab[0])\n",
    "# del en_vecs\n",
    "# torch.save(emb_enc, path/'models'/'en_enc_emb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once\n",
    "# fr_vecs = ft.load_model(str((path/'cc.fr.300.bin')))\n",
    "# emb_dec = create_emb(fr_vecs, dsrc.vocab[1])\n",
    "# del fr_vecs\n",
    "# torch.save(emb_dec, path/'models'/'fr_dec_emb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_enc = torch.load(path/'models'/'en_enc_emb.pth')\n",
    "emb_dec = torch.load(path/'models'/'fr_dec_emb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqQRNN(emb_enc, emb_dec, 256, 30, n_layers=2)\n",
    "# learn = Learner(dbch, model, loss_func=seq2seq_loss,  metrics=[seq2seq_acc, CorpusBLEU(len(dbch.vocab[1]))])\n",
    "learn = Learner(dbch, model, loss_func=seq2seq_loss,  metrics=[seq2seq_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(10,1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
